{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for implementing the distributed Finite-Time consensus experiments.\n",
    "\n",
    "Parts of this code are taken from Fernando Gama, available at: https://github.com/alelab-upenn/graph-neural-networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard libraries:\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import datetime\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch; torch.set_default_dtype(torch.float64)\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "#GNNs libraries:\n",
    "import Utils.graphTools as graphTools\n",
    "import Utils.graphAdaptiveActivations as gaActivations\n",
    "import Utils.dataTools\n",
    "import Utils.graphML as gml\n",
    "import Utils.graphML as gml\n",
    "import Modules.architectures as archit\n",
    "import Modules.model as model\n",
    "import Modules.train as train\n",
    "import Modules.loss as loss\n",
    "from Utils.miscTools import writeVarValues\n",
    "from Utils.miscTools import saveSeed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate and Save Graphs - Only Once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the parameters for generating the graph. Below I am generating an SBM graph using and saving it so I can later always use the same graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphType = 'SBM' \n",
    "nNodes = 20 # Number of nodes\n",
    "nClasses = 2 # Number of classes (i.e. number of communities)\n",
    "graphOptions = {} # Dictionary of options to pass to the graphTools.createGraph function\n",
    "graphOptions['nCommunities'] = nClasses # Number of communities\n",
    "graphOptions['probIntra'] = 0.8 # Probability of drawing edges intra communities\n",
    "graphOptions['probInter'] = 0.1 # Probability of drawing edges inter communities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I create 10 different graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allGraphs = []\n",
    "allAdjMatrix = []\n",
    "\n",
    "for i in range(0, 7):\n",
    "    G = graphTools.Graph(graphType, nNodes, graphOptions)\n",
    "    allGraphs.append(G)\n",
    "    allAdjMatrix.append(G.W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And I save them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('graphs', 'wb') as fp:\n",
    "    pickle.dump(allAdjMatrix, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create directories for saving the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphType = 'SBM_08_01'\n",
    "thisFilename = 'Finite_Time_Consensus' # General name of all related files\n",
    "saveDirRoot = 'experiments' # Relative location\n",
    "saveDir = os.path.join(saveDirRoot, thisFilename) # Where we save the results from each run\n",
    "dataDir = os.path.join('datasets','movielens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a `.txt` for saving the parameters setted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "saveDir = saveDir + '-' + graphType + '-' + today\n",
    "\n",
    "# Create directory \n",
    "if not os.path.exists(saveDir):\n",
    "    os.makedirs(saveDir)\n",
    "    \n",
    "# Create the file where all the (hyper)parameters and results will be saved.\n",
    "varsFile = os.path.join(saveDir,'hyperparameters.txt')\n",
    "with open(varsFile, 'w+') as file:\n",
    "    file.write('%s\\n\\n' % datetime.datetime.now().strftime(\"%Y/%m/%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the seeds for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   PyTorch seeds\n",
    "torchState = torch.get_rng_state()\n",
    "torchSeed = torch.initial_seed()\n",
    "\n",
    "#   Numpy seeds\n",
    "numpyState = np.random.RandomState().get_state()\n",
    "\n",
    "#   Collect all random states\n",
    "randomStates = []\n",
    "randomStates.append({})\n",
    "randomStates[0]['module'] = 'numpy'\n",
    "randomStates[0]['state'] = numpyState\n",
    "randomStates.append({})\n",
    "randomStates[1]['module'] = 'torch'\n",
    "randomStates[1]['state'] = torchState\n",
    "randomStates[1]['seed'] = torchSeed\n",
    "\n",
    "saveSeed(randomStates, saveDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If available, we use GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useGPU = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = 'ADAM' \n",
    "learningRate = 0.001 \n",
    "beta1 = 0.9 \n",
    "beta2 = 0.999 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossFunction = torch.nn.MSELoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General training parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nEpochs = 500 # Number of epochs\n",
    "batchSize = 20 # Batch size\n",
    "doLearningRateDecay = False # Learning rate decay\n",
    "learningRateDecayRate = 0.9 # Rate\n",
    "learningRateDecayPeriod = 1 # How many epochs after which update the learning rate\n",
    "validationInterval = 5 # How many training steps to do the validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save the values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeVarValues(varsFile,\n",
    "               {'trainer': trainer,\n",
    "                'learningRate': learningRate,\n",
    "                'beta1': beta1,\n",
    "                'beta2': beta2,\n",
    "                'lossFunction': lossFunction,\n",
    "                'nEpochs': nEpochs,\n",
    "                'batchSize': batchSize,\n",
    "                'doLearningRateDecay': doLearningRateDecay,\n",
    "                'learningRateDecayRate': learningRateDecayRate,\n",
    "                'learningRateDecayPeriod': learningRateDecayPeriod,\n",
    "                'validationInterval': validationInterval})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am using the functionality for training multiple models provided in Fernando Gama's code, in case we want to add more models later. But for now I am only using a local GNN with 2 layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hParamsLclGNN = {} #Hyperparameters for the Local GNN (LclGNN)\n",
    "    \n",
    "hParamsLclGNN['name'] = 'LclGNN'\n",
    "hParamsLclGNN['archit'] = archit.LocalGNN #Architecture\n",
    "    \n",
    "# Graph convolutional parameters\n",
    "hParamsLclGNN['dimNodeSignals'] = [1, 32, 32] #Features per layer\n",
    "hParamsLclGNN['nFilterTaps'] = [10, 10] #Number of filter taps per layer\n",
    "hParamsLclGNN['bias'] = True #Decide whether to include a bias term\n",
    "\n",
    "# Nonlinearity\n",
    "hParamsLclGNN['nonlinearity'] = gaActivations.MaxGAActivation\n",
    "\n",
    "# Pooling\n",
    "hParamsLclGNN['poolingFunction'] = gml.NoPool #Summarizing function -> No pooling performed\n",
    "hParamsLclGNN['nSelectedNodes'] = None # To be determined later on\n",
    "hParamsLclGNN['poolingSize'] = [1, 1] #poolingSize-hop neighborhood that is affected by the summary -> Note that \n",
    "                                        #in this code no pooling is performed\n",
    "    \n",
    "# Readout layer: local linear combination of features\n",
    "hParamsLclGNN['dimReadout'] = [1] #Dimension of the fully connected layers after the GCN layers \n",
    "        \n",
    "# Graph structure\n",
    "hParamsLclGNN['GSO'] = None #To be determined later on, based on data\n",
    "hParamsLclGNN['order'] = None #Not used because there is no pooling\n",
    "\n",
    "hParamsLclGNN2Ly = deepcopy(hParamsLclGNN)\n",
    "\n",
    "hParamsLclGNN2Ly['name'] += '2Ly' # Name of the architecture\n",
    "\n",
    "#Save Values:\n",
    "writeVarValues(varsFile, hParamsLclGNN2Ly)\n",
    "modelList += [hParamsLclGNN2Ly['name']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doPrint = True #Decide whether to print while running\n",
    "doLogging = True #Log into tensorboard\n",
    "doSaveVars = True #Save (pickle) useful variables\n",
    "doFigs = True #Plot some figures (this only works if doSaveVars is True)\n",
    "\n",
    "# Parameters:\n",
    "printInterval = 0 # After how many training steps, print the partial results\n",
    "#   0 means to never print partial results while training\n",
    "xAxisMultiplierTrain = 10 # How many training steps in between those shown in\n",
    "    # the plot, i.e., one training step every xAxisMultiplierTrain is shown.\n",
    "xAxisMultiplierValid = 1 # How many validation steps in between those shown,\n",
    "    # same as above.\n",
    "figSize = 5 # Overall size of the figure that contains the plot\n",
    "lineWidth = 2 # Width of the plot lines\n",
    "markerShape = 'o' # Shape of the markers\n",
    "markerSize = 3 # Size of the markers\n",
    "\n",
    "#Save values:\n",
    "writeVarValues(varsFile,\n",
    "               {'doPrint': doPrint,\n",
    "                'doLogging': doLogging,\n",
    "                'doSaveVars': doSaveVars,\n",
    "                'doFigs': doFigs,\n",
    "                'saveDir': saveDir,\n",
    "                'printInterval': printInterval,\n",
    "                'figSize': figSize,\n",
    "                'lineWidth': lineWidth,\n",
    "                'markerShape': markerShape,\n",
    "                'markerSize': markerSize})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if useGPU and torch.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "    \n",
    "if doPrint:\n",
    "    print(\"Device selected: %s\" % device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if doLogging:\n",
    "    # If logging is on, load the tensorboard visualizer and initialize it\n",
    "    from Utils.visualTools import Visualizer\n",
    "    logsTB = os.path.join(saveDir, 'logsTB')\n",
    "    logger = Visualizer(logsTB, name='visualResults')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set the training options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingOptions = {}\n",
    "\n",
    "if doLogging:\n",
    "    trainingOptions['logger'] = logger\n",
    "if doSaveVars:\n",
    "    trainingOptions['saveDir'] = saveDir\n",
    "if doPrint:\n",
    "    trainingOptions['printInterval'] = printInterval\n",
    "if doLearningRateDecay:\n",
    "    trainingOptions['learningRateDecayRate'] = learningRateDecayRate\n",
    "    trainingOptions['learningRateDecayPeriod'] = learningRateDecayPeriod\n",
    "trainingOptions['validationInterval'] = validationInterval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate the data, for example, by drawing from a Gaussian distribution. When it comes to the format of the data, we can choose in which way to define the labels. We can either assign one label per data point (i.e. we are interested in predicting the value of the signal at only one node) or mutiple labels per data point (i.e. we are interested in predicting the missing signal at multiple nodes).\n",
    "\n",
    "NOTE: If multiple labels are defined per data point, the size of the labels list should be the same as the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generation_finite_time_consensus_multiple_labels():\n",
    "    signals = []\n",
    "    labels = []\n",
    "    labelIDs = []\n",
    "    labels_nodes = [i for i in range(0, 100)]\n",
    "    \n",
    "    for i in range(0, 2000):\n",
    "        sg = np.random.normal(0,1,20)\n",
    "        mean_sg = np.mean(sg)\n",
    "        \n",
    "        signals.append(sg)\n",
    "        labels.append(mean_sg)\n",
    "        labelIDs.append(labels_nodes)\n",
    "\n",
    "    return signals, labels, labelIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generation_finite_time_consensus_one_label():\n",
    "    signals = []\n",
    "    labels = []\n",
    "    labelIDs = []\n",
    "    labels_nodes = [i for i in range(0, 100)]\n",
    "    \n",
    "    for i in range(0, 100):\n",
    "        sg = np.random.normal(0,1,20)\n",
    "        mean_sg = np.mean(sg)        \n",
    "        \n",
    "        for labelNode in range (0, 20):\n",
    "            labels.append(mean_sg)\n",
    "            signals.append(sg)\n",
    "            labelIDs.append(labelNode)\n",
    "            \n",
    "    return signals, labels, labelIDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a function for defining the data using the `Finite_time_consensus` class, so we can futher use the associated data methods during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(signals, labels, labelIDs, train_indexes, validation_indexes, test_indexes):\n",
    "    train_data = []\n",
    "    train_labels = []\n",
    "    train_labelIDs = []\n",
    "    for tr_index in train_indexes:\n",
    "        train_data.append(signals[tr_index])\n",
    "        train_labels.append(labels[tr_index])\n",
    "        train_labelIDs.append(labelIDs[tr_index])\n",
    "        \n",
    "        \n",
    "    validation_data = []\n",
    "    validation_labels = []\n",
    "    validation_labelIDs = []\n",
    "    for val_index in validation_indexes:\n",
    "        validation_data.append(signals[val_index])\n",
    "        validation_labels.append(labels[val_index])\n",
    "        validation_labelIDs.append(labelIDs[val_index])\n",
    "        \n",
    "    test_data = []\n",
    "    test_labels = []\n",
    "    test_labelIDs = []\n",
    "    for tst_index in test_indexes:\n",
    "        test_data.append(signals[tst_index])\n",
    "        test_labels.append(labels[tst_index])\n",
    "        test_labelIDs.append(labelIDs[tst_index])\n",
    "        \n",
    "        \n",
    "    #We define the finite time consensus data based in the already given training, validation, test splits        \n",
    "    data = Utils.dataTools.Finite_time_consensus(np.asarray(train_data), np.asarray(train_labels),\n",
    "                                                 np.asarray(train_labelIDs),np.asarray(validation_data), \n",
    "                                                 np.asarray(validation_labels), np.asarray(validation_labelIDs), \n",
    "                                                 np.asarray(test_data), np.asarray(test_labels), np.asarray(test_labelIDs))\n",
    "    \n",
    "   \n",
    "    \n",
    "    data.astype(torch.float64)\n",
    "    data.to(device)\n",
    "    \n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We always use the same graphs for training -> the ones we have defined and saved before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('graphs', 'rb') as fp:\n",
    "    allGraphs = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define permutations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate indexes up to the length of the dataset we are using (in this example, we use 2000 data points). Then we randomly permute these indexes multiple times (e.g. 10 times) such that we generate different permutations to account for the randomization involved when we split the data in train, validation and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = np.arange(0, 2000)\n",
    "allPermutations = []\n",
    "for i in range(0,10):\n",
    "    perm_indexes = np.random.permutation(indexes)\n",
    "    allPermutations.append(perm_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defince a method for initializing the model with the parameters defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(thisModel, nNodes, S, order):\n",
    "    # If a new model is to be created, it should be called for here.\n",
    "    \n",
    "    if doPrint:\n",
    "        print(\"Model initialization...\", flush = True)\n",
    "\n",
    "\n",
    "    # Get the corresponding parameter dictionary\n",
    "    hParamsDict = deepcopy(eval('hParams' + thisModel))\n",
    "\n",
    "    #Remove the 'name' and 'archit' fields from the dictionary,\n",
    "    #as these do not need to be passed to the architecture.\n",
    "    thisName = hParamsDict.pop('name')\n",
    "    callArchit = hParamsDict.pop('archit')\n",
    "\n",
    "    #Optimizer options\n",
    "    thisTrainer = trainer\n",
    "    thisLearningRate = learningRate\n",
    "    thisBeta1 = beta1\n",
    "    thisBeta2 = beta2\n",
    "\n",
    "    # Graph Shift Operaror\n",
    "    hParamsDict['GSO'] = S\n",
    "    \n",
    "    # Add the number of nodes for the no-pooling part\n",
    "    if '1Ly' in thisName:\n",
    "        hParamsDict['nSelectedNodes'] = [nNodes]\n",
    "    elif '2Ly' in thisName:\n",
    "        hParamsDict['nSelectedNodes'] = [nNodes, nNodes]\n",
    "\n",
    "    #Architecture\n",
    "    thisArchit = callArchit(**hParamsDict)\n",
    "    thisArchit.to(device)\n",
    "\n",
    "    #Optimizer\n",
    "    if thisTrainer == 'ADAM':\n",
    "        thisOptim = optim.Adam(thisArchit.parameters(),\n",
    "                                   lr = learningRate,\n",
    "                                   betas = (beta1, beta2)) #, weight_decay=1e-5)\n",
    "    elif thisTrainer == 'SGD':\n",
    "        thisOptim = optim.SGD(thisArchit.parameters(),\n",
    "                                  lr = learningRate)\n",
    "    elif thisTrainer == 'RMSprop':\n",
    "        thisOptim = optim.RMSprop(thisArchit.parameters(),\n",
    "                                      lr = learningRate, alpha = beta1)\n",
    "\n",
    "    #Loss\n",
    "    thisLossFunction = loss.adaptExtraDimensionLoss(lossFunction)\n",
    "\n",
    "\n",
    "    #Model\n",
    "    modelCreated = model.Model(thisArchit,\n",
    "                                   thisLossFunction,\n",
    "                                   thisOptim,\n",
    "                                   thisName, saveDir, order)\n",
    "    \n",
    "\n",
    "    writeVarValues(varsFile,\n",
    "                       {'name': thisName,\n",
    "                        'thisTrainer': thisTrainer,\n",
    "                        'thisLearningRate': thisLearningRate,\n",
    "                        'thisBeta1': thisBeta1,\n",
    "                        'thisBeta2': thisBeta2})\n",
    "\n",
    "        \n",
    "    return modelCreated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List in which we will store the performances for all the graphs and permutations\n",
    "graphs_perf = []\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "#Loop through all the graphs\n",
    "for GW in allGraphs:\n",
    "    print(f\"GRAPH {cnt}\")\n",
    "    \n",
    "        \n",
    "    #Define the data\n",
    "    signals, labels, labelIDs = data_generation_finite_time_consensus_one_label()\n",
    "    \n",
    "    \n",
    "    cntp = 0\n",
    "    for perm_indexes in allPermutations:\n",
    "        print(f\"PERM {cntp}\")\n",
    "        \n",
    "        #Save the results from the runs\n",
    "        thisFilename = 'finiteTimeConsensus graph ' + str(cnt) + ' perm ' + str(cntp) # This is the general name of all related files\n",
    "        saveDir2 = os.path.join(saveDir, thisFilename) # Dir where to save all the results from each run\n",
    "        if not os.path.exists(saveDir2):\n",
    "            os.makedirs(saveDir2)\n",
    "        trainingOptions['saveDir2'] = saveDir2\n",
    "\n",
    "        #Split the data\n",
    "        train_indexes, validation_indexes, test_indexes = np.split(perm_indexes, [int(.8 * len(perm_indexes)), int(.9 * len(perm_indexes))])\n",
    "        data = getData(signals, labels, labelIDs, train_indexes, validation_indexes, test_indexes)\n",
    "        \n",
    "        \n",
    "        #Initialize the models dictionary\n",
    "        modelsGNN = {}\n",
    "        thisName = modelList[0]\n",
    "\n",
    "        \n",
    "        #Compute the shift operator for the current graph -> normalized adjacency\n",
    "        GS = GW\n",
    "        # Get the largest eigenvalue of the weighted adjacency matrix\n",
    "        EW, VW = graphTools.computeGFT(GW, order = 'totalVariation')\n",
    "        eMax = np.max(EW)\n",
    "        #Ordering\n",
    "        S, order = graphTools.permIdentity(GS/eMax)\n",
    "        \n",
    "\n",
    "        #Initialize the local GNN model        \n",
    "        LocalGNN = initialize_model(modelList[0], nNodes, S, order)\n",
    "        \n",
    "        #Add it to the dictionary\n",
    "        modelsGNN[thisName] = LocalGNN\n",
    "        \n",
    "        \n",
    "        #Train the model\n",
    "        train.MultipleModels(modelsGNN, data,\n",
    "                     nEpochs = nEpochs, batchSize = batchSize,\n",
    "                     **trainingOptions)\n",
    "        \n",
    "        \n",
    "    \n",
    "        ###################\n",
    "        ### EVALUATION ###\n",
    "        ##################\n",
    "        \n",
    "        xTest, yTest, idsTest = data.getSamples('test')\n",
    "        xTest = xTest.unsqueeze(1)\n",
    "        xTest = xTest.to(device)\n",
    "        yTest = yTest.to(device)\n",
    "    \n",
    "    \n",
    "    \n",
    "        for key in modelsGNN.keys():\n",
    "            # Update order and adapt dimensions (this data has one input feature,\n",
    "            # so we need to add that dimension; make it from B x N to B x F x N)\n",
    "            #xTestOrdered = xTest[:,modelsGNN[key].order]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Process the samples\n",
    "                yHatTest = modelsGNN[key].archit.singleNodeForward(xTest, idsTest)\n",
    "            \n",
    "                # We compute the accuracy\n",
    "                thisAcc = data.evaluate(yHatTest, yTest)\n",
    "        \n",
    "        #Record and print the performance\n",
    "        graphs_perf.append(thisAcc)\n",
    "        \n",
    "        cntp += 1\n",
    "        \n",
    "    cnt += 1\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the average performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_perf = 0\n",
    "\n",
    "for i in range(0,100):\n",
    "    sum_perf += graphs_accuracies[i][j]\n",
    "        \n",
    "avg_perf = sum_perf/100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
