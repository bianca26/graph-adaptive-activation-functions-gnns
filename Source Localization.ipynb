{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for implementing the distributed Source Localization experiments.\n",
    "\n",
    "Parts of this code are taken from Fernando Gama, available at: https://github.com/alelab-upenn/graph-neural-networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard libraries:\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import datetime\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch; torch.set_default_dtype(torch.float64)\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "#GNNs libraries\n",
    "import Utils.graphTools as graphTools\n",
    "import Utils.dataTools\n",
    "import Utils.graphML as gml\n",
    "import Utils.graphAdaptiveActivations as gaActivations\n",
    "import Modules.architectures as archit\n",
    "import Modules.model as model\n",
    "import Modules.train as train\n",
    "import Modules.loss as loss\n",
    "\n",
    "#Miscellaneous libraries\n",
    "from Utils.miscTools import writeVarValues\n",
    "from Utils.miscTools import saveSeed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up the directors for writing the logs and the results from each run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphType = 'SBM'\n",
    "thisFilename = 'SourceLocalization' #This is the general name of all related files\n",
    "saveDirRoot = 'experiments_sourceLocalization'\n",
    "saveDir = os.path.join(saveDirRoot, thisFilename) # Dir where to save all the results from each run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate next a `.txt` file for saving the values used for the setting parameters for an easier reference. We append date and time as well for each of the runs, to avoid overwriting when multiple experiments are run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "saveDir = saveDir + '-' + graphType + '-' + today\n",
    "\n",
    "# If the directory does not exist, create it\n",
    "if not os.path.exists(saveDir):\n",
    "    os.makedirs(saveDir)\n",
    "    \n",
    "# Create the file where the setting parameters will be saved.\n",
    "varsFile = os.path.join(saveDir,'hyperparameters.txt')\n",
    "\n",
    "with open(varsFile, 'w+') as file:\n",
    "    file.write('%s\\n\\n' % datetime.datetime.now().strftime(\"%Y/%m/%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save seeds for further reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   PyTorch seeds\n",
    "torchState = torch.get_rng_state()\n",
    "torchSeed = torch.initial_seed()\n",
    "#   Numpy seeds\n",
    "numpyState = np.random.RandomState().get_state()\n",
    "#   Collect all random states\n",
    "randomStates = []\n",
    "randomStates.append({})\n",
    "randomStates[0]['module'] = 'numpy'\n",
    "randomStates[0]['state'] = numpyState\n",
    "randomStates.append({})\n",
    "randomStates[1]['module'] = 'torch'\n",
    "randomStates[1]['state'] = torchState\n",
    "randomStates[1]['seed'] = torchSeed\n",
    "\n",
    "saveSeed(randomStates, saveDir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If GPU is available, we use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useGPU = True "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the number of nodes in the SBM graph, as well as the number of classes (i.e., number of communities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nNodes = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nClasses = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set next the training options, such as optimizer, loss function, number of epochs, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = 'ADAM' # Options: 'SGD', 'ADAM', 'RMSprop'\n",
    "learningRate = 0.001 # In all options\n",
    "beta1 = 0.9 # beta1 if 'ADAM', alpha if 'RMSprop'\n",
    "beta2 = 0.999 # ADAM option only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossFunction = torch.nn.CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nEpochs = 800 # Number of epochs\n",
    "batchSize = 100 # Batch size\n",
    "doLearningRateDecay = False # Learning rate decay\n",
    "learningRateDecayRate = 0.9 # Rate\n",
    "learningRateDecayPeriod = 1 # How many epochs after which update the learning rate\n",
    "validationInterval = 20 # How many training steps to do the validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save next the values we set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeVarValues(varsFile,\n",
    "               {'trainer': trainer,\n",
    "                'learningRate': learningRate,\n",
    "                'beta1': beta1,\n",
    "                'beta2': beta2,\n",
    "                'lossFunction': lossFunction,\n",
    "                'nEpochs': nEpochs,\n",
    "                'batchSize': batchSize,\n",
    "                'doLearningRateDecay': doLearningRateDecay,\n",
    "                'learningRateDecayRate': learningRateDecayRate,\n",
    "                'learningRateDecayPeriod': learningRateDecayPeriod,\n",
    "                'validationInterval': validationInterval})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the architecture we will use. Multiple architecture can be defined and included in the model list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelList = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will employ the Local GNN defined by Fernando Gama (available at https://github.com/alelab-upenn/graph-neural-networks), together with our proposed max graph-adaptive activation function. We perform no pooling. The model has two layers.\n",
    "\n",
    "In the end, we save the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hParamsLclGNN = {} # Hyperparameters (hParams) for the Local GNN (LclGNN)\n",
    "\n",
    "# Chosen architecture is the Local GNN\n",
    "hParamsLclGNN['name'] = 'LclGNN'\n",
    "hParamsLclGNN['archit'] = archit.LocalGNN\n",
    "    \n",
    "# Graph convolutional parameters\n",
    "hParamsLclGNN['dimNodeSignals'] = [1, 5, 5] # Features per layer\n",
    "hParamsLclGNN['nFilterTaps'] = [2, 2] # Number of filter taps per layer\n",
    "hParamsLclGNN['bias'] = True # Decide whether to include a bias term\n",
    "\n",
    "# Nonlinearity\n",
    "hParamsLclGNN['nonlinearity'] = gaActivations.MaxGAActivation\n",
    "\n",
    "# Pooling\n",
    "hParamsLclGNN['poolingFunction'] = gml.NoPool # Summarizing function\n",
    "hParamsLclGNN['nSelectedNodes'] = None # To be determined later on\n",
    "hParamsLclGNN['poolingSize'] = [1, 1] # poolingSize-hop neighborhood that is affected by the summary\n",
    "    \n",
    "# Readout layer: local linear combination of features\n",
    "hParamsLclGNN['dimReadout'] = [nClasses] # Dimension of the fully connected layers\n",
    "        # after the GCN layers (map); this fully connected layer is applied only\n",
    "        # at each node, without any further exchanges nor considering all nodes\n",
    "        # at once, making the architecture entirely local.\n",
    "        \n",
    "# Graph structure\n",
    "hParamsLclGNN['GSO'] = None # To be determined later on\n",
    "hParamsLclGNN['order'] = None # Not used because there is no pooling\n",
    "\n",
    "hParamsLclGNN2Ly = deepcopy(hParamsLclGNN)\n",
    "\n",
    "hParamsLclGNN2Ly['name'] += '2Ly' # Name of the architecture\n",
    "\n",
    "#Save Values:\n",
    "writeVarValues(varsFile, hParamsLclGNN2Ly)\n",
    "modelList += [hParamsLclGNN2Ly['name']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decide on the logging variable, whether we want to print statemets during running and save variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doPrint = True # Decide whether to print stuff while running\n",
    "doLogging = True # Log into tensorboard\n",
    "doSaveVars = True # Save (pickle) useful variables\n",
    "doFigs = True # Plot some figures (this only works if doSaveVars is True)\n",
    "\n",
    "\n",
    "#\\\\\\ Save values:\n",
    "writeVarValues(varsFile,\n",
    "               {'doPrint': doPrint,\n",
    "                'doSaveVars': doSaveVars})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the processing unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if useGPU and torch.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    \n",
    "# Notify:\n",
    "if doPrint:\n",
    "    print(\"Device selected: %s\" % device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a dictionary to pass to the train function with all the options set before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingOptions = {}\n",
    "\n",
    "if doSaveVars:\n",
    "    trainingOptions['saveDir'] = saveDir\n",
    "if doPrint:\n",
    "    trainingOptions['printInterval'] = printInterval\n",
    "if doLearningRateDecay:\n",
    "    trainingOptions['learningRateDecayRate'] = learningRateDecayRate\n",
    "    trainingOptions['learningRateDecayPeriod'] = learningRateDecayPeriod\n",
    "trainingOptions['validationInterval'] = validationInterval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function for generating the data for the Source Localization problem on the SBM graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generation_SBM(G, sourceNodes, nodesPerCommunity, numberOfCommunities, tMax = None):\n",
    "    # If no tMax is specified, set it the maximum possible.\n",
    "    if tMax == None:\n",
    "        tMax = G.N\n",
    "    \n",
    "    # Get the largest eigenvalue of the weighted adjacency matrix\n",
    "    EW, VW = graphTools.computeGFT(G.W, order = 'totalVariation')\n",
    "    eMax = np.max(EW)\n",
    "    # Normalize the matrix so that it doesn't explode\n",
    "    Wnorm = G.W / eMax\n",
    "    \n",
    "    \n",
    "    # Since the signals are generated as W^t * delta, this reduces to the\n",
    "    # selection of a column of W^t (the column corresponding to the source\n",
    "    # node). Therefore, we generate an array of size tMax x N x N with all\n",
    "    # the powers of the matrix, and then we just simply select the\n",
    "    # corresponding column for the corresponding time\n",
    "    lastWt = np.eye(G.N, G.N)\n",
    "    Wt = lastWt.reshape([1, G.N, G.N])\n",
    "    for t in range(1,tMax):\n",
    "        lastWt = lastWt @ Wnorm\n",
    "        Wt = np.concatenate((Wt, lastWt.reshape([1, G.N, G.N])), axis = 0)\n",
    "    \n",
    "    #We next define the signals and the labels (communities that generated the signal)\n",
    "    signals = []\n",
    "    labels = []\n",
    "    \n",
    "    for source in sourceNodes:\n",
    "        \n",
    "        source_label = 0\n",
    "        \n",
    "        for c in range(0, numberOfCommunities):\n",
    "            if source in nodesPerCommunity[c]:\n",
    "                source_label = c\n",
    "                break\n",
    "        \n",
    "        for diffusionTime in range(0, tMax):\n",
    "            signals.append(Wt[diffusionTime, :, source])\n",
    "            labels.append(source_label)\n",
    "    \n",
    "    #Check shape\n",
    "    print(Wt.shape)\n",
    "    \n",
    "    return signals, labels "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function that splits the data in train, validation and test sets based on the indices we provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(signals, labels, train_indexes, validation_indexes, test_indexes, labelID):\n",
    "    train_data = []\n",
    "    train_labels = []\n",
    "    train_labelIDs = []\n",
    "    for tr_index in train_indexes:\n",
    "        train_data.append(signals[tr_index])\n",
    "        train_labels.append(labels[tr_index])\n",
    "        train_labelIDs.append(labelID)\n",
    "        \n",
    "        \n",
    "    validation_data = []\n",
    "    validation_labels = []\n",
    "    validation_labelIDs = []\n",
    "    for val_index in validation_indexes:\n",
    "        validation_data.append(signals[val_index])\n",
    "        validation_labels.append(labels[val_index])\n",
    "        validation_labelIDs.append(labelID)\n",
    "        \n",
    "    test_data = []\n",
    "    test_labels = []\n",
    "    test_labelIDs = []\n",
    "    for tst_index in test_indexes:\n",
    "        test_data.append(signals[tst_index])\n",
    "        test_labels.append(labels[tst_index])\n",
    "        test_labelIDs.append(labelID)\n",
    "    \n",
    "    #Check the split\n",
    "    print(f\"Train {len(train_data)}\")\n",
    "    print(f\"Test {len(test_data)}\")\n",
    "    print(f\"Validation {len(validation_data)}\")\n",
    "        \n",
    "    \n",
    "    #Generate the source localization data, given the split\n",
    "    data = Utils.dataTools.SourceLocalization(np.asarray(train_data), np.asarray(train_labels), np.asarray(train_labelIDs),\n",
    "                                          np.asarray(validation_data), np.asarray(validation_labels), np.asarray(validation_labelIDs),\n",
    "                                          np.asarray(test_data), np.asarray(test_labels), np.asarray(test_labelIDs))\n",
    "\n",
    "    data.astype(torch.float64)\n",
    "    data.to(device)\n",
    "    \n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read and check the shape of the graphs, perviously generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('graphs', 'rb') as fp:\n",
    "    allGraphs = pickle.load(fp)\n",
    "    \n",
    "print(np.shape(allGraphs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initialization And Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the model previously defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(thisModel, nNodes, S, order):\n",
    "    \n",
    "    # Get the corresponding parameter dictionary\n",
    "    hParamsDict = deepcopy(eval('hParams' + thisModel))\n",
    "\n",
    "    #Remove the 'name' and 'archit' fields from the dictionary,\n",
    "    #as these do not need to be passed to the architecture.\n",
    "    thisName = hParamsDict.pop('name')\n",
    "    callArchit = hParamsDict.pop('archit')\n",
    "\n",
    "\n",
    "    #Optimizer options\n",
    "    thisTrainer = trainer\n",
    "    thisLearningRate = learningRate\n",
    "    thisBeta1 = beta1\n",
    "    thisBeta2 = beta2\n",
    "\n",
    "   \n",
    "    # Graph Shift Operaror\n",
    "    hParamsDict['GSO'] = S\n",
    "    \n",
    "    # Add the number of nodes for the no-pooling part\n",
    "    if '1Ly' in thisName:\n",
    "        hParamsDict['nSelectedNodes'] = [nNodes]\n",
    "    elif '2Ly' in thisName:\n",
    "        hParamsDict['nSelectedNodes'] = [nNodes, nNodes]\n",
    "\n",
    "    #Architecture\n",
    "    thisArchit = callArchit(**hParamsDict)\n",
    "    thisArchit.to(device)\n",
    "\n",
    "    #Optimizer\n",
    "    if thisTrainer == 'ADAM':\n",
    "        thisOptim = optim.Adam(thisArchit.parameters(),\n",
    "                                   lr = learningRate,\n",
    "                                   betas = (beta1, beta2)) #, weight_decay=1e-5)\n",
    "    elif thisTrainer == 'SGD':\n",
    "        thisOptim = optim.SGD(thisArchit.parameters(),\n",
    "                                  lr = learningRate)\n",
    "    elif thisTrainer == 'RMSprop':\n",
    "        thisOptim = optim.RMSprop(thisArchit.parameters(),\n",
    "                                      lr = learningRate, alpha = beta1)\n",
    "\n",
    "    #Loss\n",
    "    thisLossFunction = loss.adaptExtraDimensionLoss(lossFunction)\n",
    "    #thisLossFunction = lossFunction\n",
    "\n",
    "    #Model\n",
    "    modelCreated = model.Model(thisArchit,\n",
    "                                   thisLossFunction,\n",
    "                                   thisOptim,\n",
    "                                   thisName, saveDir, order)\n",
    "    \n",
    "\n",
    "    writeVarValues(varsFile,\n",
    "                       {'name': thisName,\n",
    "                        'thisTrainer': thisTrainer,\n",
    "                        'thisLearningRate': thisLearningRate,\n",
    "                        'thisBeta1': thisBeta1,\n",
    "                        'thisBeta2': thisBeta2})\n",
    "\n",
    "        \n",
    "    return modelCreated\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we initialized the model, we can perform the trianing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_graph_accuracies = []\n",
    "\n",
    "#Maximum time of diffusion\n",
    "tMax = 30\n",
    "\n",
    "cnt = 0\n",
    "for G in allGraphs:\n",
    "    print(f\"GRAPH {cnt}\")\n",
    "    pred_node_accuracies = []\n",
    "        \n",
    "    #We retrieve the source nodes, the nodes per community and the nodes for which we want to perform the prediction\n",
    "    sourceNodes, nodesPerCommunity, prediction_nodes = graphTools.computeSourceNodes(G.W, nClasses)\n",
    "\n",
    "    #Given these nodes, we generate the signals and labels\n",
    "    signals, labels = data_generation_SBM(G, sourceNodes, nodesPerCommunity, nClasses, tMax)\n",
    "    \n",
    "    #Generate random permutation and retrieve the train, validation and test indexes\n",
    "    indexes = np.arange(0, len(signals))\n",
    "    perm_indexes = np.random.permutation(indexes)\n",
    "    train_indexes, validation_indexes, test_indexes = np.split(perm_indexes, [int(.8 * len(perm_indexes)), int(.9 * len(perm_indexes))])\n",
    "\n",
    "    #Used for file name\n",
    "    cnt_pred = 0\n",
    "    \n",
    "    #We perform the training and evaluation for each of the nodes chosen for prediction\n",
    "    for labelID in prediction_nodes:  \n",
    "        \n",
    "        #Generate the files for saving\n",
    "        thisFilename = 'finiteTimeConsensus graph ' + str(cnt) + ' prediction node ' + str(cnt_pred) \n",
    "        saveDir2 = os.path.join(saveDir, thisFilename) # Director where to save all the results from each run\n",
    "        #Create the director\n",
    "        if not os.path.exists(saveDir2):\n",
    "            os.makedirs(saveDir2)\n",
    "        trainingOptions['saveDir'] = saveDir2\n",
    "        \n",
    "        #Get the data\n",
    "        data = getData(signals, labels, train_indexes, validation_indexes, test_indexes, labelID)\n",
    "        \n",
    "        \n",
    "        #Initialize the models dictionary\n",
    "        modelsGNN = {}\n",
    "        thisName = modelList[0]\n",
    "        \n",
    "       \n",
    "        #Ordering\n",
    "        G.computeGFT()\n",
    "        S, order = graphTools.permIdentity(G.S/np.max(np.diag(G.E)))\n",
    "        \n",
    "        #Initialize the local GNN model        \n",
    "        LocalGNN = initialize_model(modelList[0], nNodes, S, order)\n",
    "        \n",
    "        #Add it to the dictionary\n",
    "        modelsGNN[thisName] = LocalGNN\n",
    "        \n",
    "        #Train the model\n",
    "        train.MultipleModels(modelsGNN, data,\n",
    "                     nEpochs = nEpochs, batchSize = batchSize,\n",
    "                     **trainingOptions)\n",
    "        \n",
    "        #EVALUATION\n",
    "        \n",
    "        #Get the test samples\n",
    "        xTest, yTest, idsTest = data.getSamples('test')\n",
    "        xTest = xTest.unsqueeze(1)\n",
    "        xTest = xTest.to(device)\n",
    "        yTest = yTest.to(device)\n",
    "    \n",
    "    \n",
    "    \n",
    "        for key in modelsGNN.keys():\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Process the samples\n",
    "                yHatTest = modelsGNN[key].archit.singleNodeForward(xTest, idsTest)\n",
    "                # We compute the accuracy\n",
    "                accuracy = data.evaluate(yHatTest, yTest)\n",
    "\n",
    "        #Save the performance\n",
    "        all_graph_accuracies.append(accuracy)\n",
    "        \n",
    "        #makePlots(saveDir, modelList)\n",
    "        cnt_pred += 1\n",
    "        \n",
    "        \n",
    "    cnt += 1\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print results\n",
    "print(all_graph_accuracies)\n",
    "print(f\"mean {torch.mean(torch.tensor(all_graph_accuracies))}\")\n",
    "print(f\"std {torch.std(torch.tensor(all_graph_accuracies))}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
